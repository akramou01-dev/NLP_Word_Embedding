{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c94f1f9a",
   "metadata": {},
   "source": [
    "in this notebook we will classify a clients sentiments reviews based on some training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b2bcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Flatten , Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "af8a8ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = [\n",
    "    'Nice service',\n",
    "    'Delisious food', \n",
    "    \"Amazing place to eat\",\n",
    "    \"Not satisfied\",\n",
    "    \"poor service\",\n",
    "    \"Just love it!\",\n",
    "    \"Needs improvements\",\n",
    "    \"Bad food quality\",\n",
    "    \"Never come back here!\",\n",
    "    \"too good\"\n",
    "]\n",
    "\n",
    "labels= np.array([1,1,1,0,0,1,0,0,0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5442dfa1",
   "metadata": {},
   "source": [
    "the one_hot fonction encodes a text into a list of word indexes (integers) of size 'n' that we specify as second argument of the fonction.\n",
    "\n",
    "it gives us unique integers (between 0 and n where n is the second argument of the fonction) for each word in the input_text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5ca8bbaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 3, 8, 9, 4]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot(\"Hey There, How are you !\",10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "6e6879dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11, 24], [42, 44], [1, 41, 21, 45], [7, 40], [27, 24], [36, 1, 25], [39, 8], [2, 44, 10], [6, 35, 29, 8], [4, 26]]\n"
     ]
    }
   ],
   "source": [
    "# now we will encode all the reviews into integers \n",
    "vocab_size =50\n",
    "encoded_reviews = [one_hot(words,vocab_size) for words in reviews]\n",
    "print(encoded_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af51e18",
   "metadata": {},
   "source": [
    "we need to padd every sequence of words into the len of the largest sequence in the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c2a2b3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11 24  0  0]\n",
      " [42 44  0  0]\n",
      " [ 1 41 21 45]\n",
      " [ 7 40  0  0]\n",
      " [27 24  0  0]\n",
      " [36  1 25  0]\n",
      " [39  8  0  0]\n",
      " [ 2 44 10  0]\n",
      " [ 6 35 29  8]\n",
      " [ 4 26  0  0]]\n"
     ]
    }
   ],
   "source": [
    "max_len = max([len(w.split()) for w in reviews])\n",
    "padded_reviews = pad_sequences(encoded_reviews, maxlen=max_len, padding='post')\n",
    "print(padded_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0104ba75",
   "metadata": {},
   "source": [
    "now all the sequences are padded, and they all have the same size \n",
    "\n",
    "the next step is to create the model and for this we must define the embeded vector size, which is the size of the ouput vectors of the Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "6b7d5f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 4, 8)              400       \n",
      "_________________________________________________________________\n",
      "flatten_20 (Flatten)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 433\n",
      "Trainable params: 433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embaded_vector_size = 5\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = vocab_size,output_dim= 8, input_length = max_len,name = 'embedding'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1,activation=\"sigmoid\"))\n",
    "model.compile(optimizer = 'adam',loss=\"binary_crossentropy\",metrics =[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "bf895e47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x24372cd6760>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(padded_reviews, labels, epochs=50, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b832e8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:9 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000024370194790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      "\n",
      "The loss : 0.5743067860603333, accuracy : 1.0\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(padded_reviews,labels,verbose=0)\n",
    "print(f\"\\n\\nThe loss : {loss}, accuracy : {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "3ef456b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.get_layer(\"embedding\").get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "4373f3df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02778094, -0.02430862, -0.03690113, -0.07403668, -0.00286919,\n",
       "       -0.0463014 , -0.07022117, -0.05354059], dtype=float32)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "cf791dee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0722071 , -0.09330942, -0.03600999, -0.07076837, -0.06134059,\n",
       "       -0.01724471, -0.01825108, -0.06911809], dtype=float32)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[42]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439f081a",
   "metadata": {},
   "source": [
    "here we see that the 2 words nice and delicious have almost the same vectors and that because they are similaire.\n",
    "\n",
    "so for a big dataset can perform the weights, so that all the similaire values have the same vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6fee49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
